{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Ranker Recommended system"
      ],
      "metadata": {
        "id": "xjmHjVp4Kx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import notebook\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import xgboost as xgb\n",
        "from typing import Optional, Tuple\n"
      ],
      "metadata": {
        "id": "u4dogGv-UvT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data"
      ],
      "metadata": {
        "id": "gUlL9CI_VXuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data from Kaggle\n",
        "article_pth = '/content/articles.csv'\n",
        "transaction_pth = '/content/transactions_train.csv'\n",
        "customer_pth = '/content/customers.csv'\n",
        "\n",
        "# feature extraction\n",
        "adv_user_feature_pth = '/content/cust_features'\n",
        "adv_item_feature_pth = '/content/item_features.parquet'\n"
      ],
      "metadata": {
        "id": "ooAPydNTxk5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cust feature\n",
        "user_features = pd.read_parquet(adv_user_feature_pth)\n",
        "user_features[['club_member_status', 'fashion_news_frequency']] = (\n",
        "    user_features[['club_member_status', 'fashion_news_frequency']]\n",
        "    .apply(lambda x: pd.factorize(x)[0])\n",
        ").astype('int8')\n",
        "user_features = user_features.reset_index()\n",
        "customer_df = pd.read_csv(customer_pth)\n",
        "\n",
        "# item feature\n",
        "article_df = pd.read_csv(article_pth)\n",
        "article_df['article_id'] = '0' + article_df['article_id'].astype(str)\n",
        "item_features = pd.read_parquet(adv_item_feature_pth)\n",
        "item_features = item_features.reset_index()\n",
        "item_features['article_id'] = '0' + item_features['article_id'].astype(str)\n",
        "\n",
        "# transaction data\n",
        "transaction_df = pd.read_csv(transaction_pth)\n",
        "transaction_df['t_dat'] = pd.to_datetime(transaction_df['t_dat'])\n",
        "transaction_df['article_id'] = '0' + transaction_df['article_id'].astype(str)\n",
        "transaction_df['week'] = 104 - (transaction_df.t_dat.max() - transaction_df.t_dat).dt.days // 7\n"
      ],
      "metadata": {
        "id": "Rd-LpP8gVcrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NegativeSampling"
      ],
      "metadata": {
        "id": "eFDPHZFXnNg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class NegativeSampling:\n",
        "\n",
        "    def __init__(self, transaction_df: pd.DataFrame, train_inteval: int = 10):\n",
        "        self.transaction_df = transaction_df\n",
        "        self.train_trans, self.valid_trans = self._train_valid_split(\n",
        "            transaction_df, train_inteval\n",
        "        )\n",
        "        self.valid_week = transaction_df.week.max()\n",
        "\n",
        "    def _train_valid_split(\n",
        "        self, transaction_df: pd.DataFrame, train_interval: int\n",
        "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        valid_trans = transaction_df[transaction_df.week == transaction_df.week.max()]\n",
        "        train_trans = transaction_df[\n",
        "            (transaction_df.week != transaction_df.week.max())\n",
        "            & (transaction_df.week > transaction_df.week.max() - train_interval)\n",
        "        ]\n",
        "        return train_trans, valid_trans\n",
        "\n",
        "    def create_data_with_neg_sample(\n",
        "        self,\n",
        "        extra_user_features: Optional[pd.DataFrame] = None,\n",
        "        extra_item_features: Optional[pd.DataFrame] = None,\n",
        "    ) -> pd.DataFrame:\n",
        "        # combine transaction and negative samples (candidates)\n",
        "        train_trans = self.train_trans.copy()\n",
        "        train_trans[\"purchased\"] = 1\n",
        "\n",
        "        candidates_last_purchase = self._find_last_purchase()\n",
        "        candidates_bestsellers, bestsellers_previous_week = self._find_bestsellers()\n",
        "\n",
        "        self.bestsellers_previous_week = bestsellers_previous_week\n",
        "\n",
        "        data = pd.concat(\n",
        "            [train_trans, candidates_last_purchase, candidates_bestsellers]\n",
        "        )\n",
        "        data.purchased.fillna(0, inplace=True)\n",
        "        data.drop_duplicates([\"customer_id\", \"article_id\", \"week\"], inplace=True)\n",
        "        data = pd.merge(\n",
        "            data,\n",
        "            bestsellers_previous_week[[\"week\", \"article_id\", \"bestseller_rank\"]],\n",
        "            on=[\"week\", \"article_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "\n",
        "        data = data[data.week != data.week.min()]\n",
        "        data.bestseller_rank.fillna(999, inplace=True)\n",
        "\n",
        "        if extra_item_features is not None:\n",
        "            data = pd.merge(data, extra_item_features, on=\"article_id\", how=\"left\")\n",
        "        if extra_user_features is not None:\n",
        "            data = pd.merge(data, extra_user_features, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "        data.sort_values([\"week\", \"customer_id\"], inplace=True)\n",
        "        data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _find_last_purchase(self) -> pd.DataFrame:\n",
        "        c2weeks = self.transaction_df.groupby(\"customer_id\")[\"week\"].unique()\n",
        "\n",
        "        c2weeks2shifted_weeks = {}\n",
        "        for c_id, weeks in c2weeks.items():\n",
        "            c2weeks2shifted_weeks[c_id] = {}\n",
        "            for i in range(weeks.shape[0] - 1):\n",
        "                c2weeks2shifted_weeks[c_id][weeks[i]] = weeks[i + 1]\n",
        "\n",
        "            c2weeks2shifted_weeks[c_id][weeks[-1]] = self.valid_week\n",
        "\n",
        "        candidates_last_purchase = self.train_trans.copy()\n",
        "        weeks = []\n",
        "        for i, (c_id, week) in enumerate(\n",
        "            zip(self.train_trans[\"customer_id\"], self.train_trans[\"week\"])\n",
        "        ):\n",
        "            weeks.append(c2weeks2shifted_weeks[c_id][week])\n",
        "\n",
        "        candidates_last_purchase.week = weeks\n",
        "\n",
        "        return candidates_last_purchase\n",
        "\n",
        "    def _find_bestsellers(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        mean_price = self.train_trans.groupby([\"week\", \"article_id\"])[\"price\"].mean()\n",
        "        sales = (\n",
        "            self.train_trans.groupby(\"week\")[\"article_id\"]\n",
        "            .value_counts()\n",
        "            .groupby(\"week\")\n",
        "            .rank(method=\"dense\", ascending=False)\n",
        "            .groupby(\"week\")\n",
        "            .head(12)\n",
        "            .rename(\"bestseller_rank\")\n",
        "            .astype(\"int8\")\n",
        "        )\n",
        "\n",
        "        bestsellers_previous_week = pd.merge(\n",
        "            sales, mean_price, on=[\"week\", \"article_id\"]\n",
        "        ).reset_index()\n",
        "        bestsellers_previous_week.week += 1\n",
        "\n",
        "        unique_transactions = (\n",
        "            self.train_trans.groupby([\"week\", \"customer_id\"])\n",
        "            .head(1)\n",
        "            .drop(columns=[\"article_id\", \"price\"])\n",
        "        ).copy()\n",
        "\n",
        "        candidates_bestsellers = pd.merge(\n",
        "            unique_transactions,\n",
        "            bestsellers_previous_week,\n",
        "            on=\"week\",\n",
        "        )\n",
        "\n",
        "        valid_set_transactions = unique_transactions.drop_duplicates(\n",
        "            \"customer_id\"\n",
        "        ).reset_index(drop=True)\n",
        "        valid_set_transactions.week = self.valid_week\n",
        "\n",
        "        candidates_bestsellers_valid_week = pd.merge(\n",
        "            valid_set_transactions, bestsellers_previous_week, on=\"week\"\n",
        "        )\n",
        "\n",
        "        candidates_bestsellers = pd.concat(\n",
        "            [candidates_bestsellers, candidates_bestsellers_valid_week]\n",
        "        )\n",
        "        candidates_bestsellers.drop(columns=\"bestseller_rank\", inplace=True)\n",
        "\n",
        "        return candidates_bestsellers, bestsellers_previous_week\n"
      ],
      "metadata": {
        "id": "QD_c9qZsS_C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "9goxcLxBxzlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "neg_sampling = NegativeSampling(\n",
        "    transaction_df=transaction_df, train_inteval=10\n",
        ")\n",
        "data = neg_sampling.create_data_with_neg_sample(\n",
        "    extra_user_features=user_features,\n",
        "    extra_item_features=item_features,\n",
        ")"
      ],
      "metadata": {
        "id": "oNwBG4PwFsCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/valid\n",
        "valid_week = neg_sampling.valid_week\n",
        "train = data[data.week != valid_week]\n",
        "valid = data[data.week==valid_week].drop_duplicates(['customer_id', 'article_id', 'sales_channel_id']).copy()\n",
        "\n",
        "train_X = train.drop(columns=['purchased', 't_dat', 'price', 'sales_channel_id', 'customer_id', 'article_id', 'week'])\n",
        "train_y = train['purchased']\n",
        "valid_X = valid.drop(columns=['purchased', 't_dat', 'price', 'sales_channel_id', 'customer_id', 'article_id', 'week'])"
      ],
      "metadata": {
        "id": "W3IBaen_WqZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trainning"
      ],
      "metadata": {
        "id": "99ndxJaMXz4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####XGBRanker model\n"
      ],
      "metadata": {
        "id": "w5B9sSXYgYMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make group for training\n",
        "train_baskets = train.groupby(\n",
        "    ['week', 'customer_id']\n",
        ")['article_id'].count().values\n",
        "\n",
        "# Parameter\n",
        "xgb_ranker = xgb.XGBRanker(\n",
        "    eta= 0.5,\n",
        "    max_depth= 10,\n",
        "    n_estimators= 100\n",
        ")\n",
        "# Trainning\n",
        "xgb_ranker.fit(\n",
        "    train_X,\n",
        "    train_y,\n",
        "    group=train_baskets,\n",
        ")"
      ],
      "metadata": {
        "id": "9JLnLs1VgTug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "30be1f52-2816-46f0-f1d4-28b18e32dc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
              "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "          early_stopping_rounds=None, enable_categorical=False, eta=0.5,\n",
              "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
              "          importance_type=None, interaction_constraints=None,\n",
              "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "          max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
              "          max_leaves=None, min_child_weight=None, missing=nan,\n",
              "          monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
              "          n_jobs=None, num_parallel_tree=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
              "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "          early_stopping_rounds=None, enable_categorical=False, eta=0.5,\n",
              "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
              "          importance_type=None, interaction_constraints=None,\n",
              "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "          max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
              "          max_leaves=None, min_child_weight=None, missing=nan,\n",
              "          monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
              "          n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n",
              "          colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "          early_stopping_rounds=None, enable_categorical=False, eta=0.5,\n",
              "          eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
              "          importance_type=None, interaction_constraints=None,\n",
              "          learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "          max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
              "          max_leaves=None, min_child_weight=None, missing=nan,\n",
              "          monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
              "          n_jobs=None, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction & Measurement"
      ],
      "metadata": {
        "id": "kITPXYawX7ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Make MAP metrics function"
      ],
      "metadata": {
        "id": "QqpIHvu4Zsd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apk(actual, predicted, k=12):\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "    # Lặp qua các dự đoán\n",
        "    for i, p in enumerate(predicted):\n",
        "        # nếu dự đoán có trong actual và chưa được dự đoán trước đó\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            # tăng num_hits\n",
        "            num_hits += 1.0\n",
        "            # tính điểm ap tại phần tử đó\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    # trả về điểm trung bình ap cho dãy dự đoán\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def mapk(\n",
        "    measure_df: pd.DataFrame,\n",
        "    pred_col: str = \"prediction\",\n",
        "    ground_true_col: str = \"ground_true\",\n",
        "    k=12,\n",
        "):\n",
        "    apks = []\n",
        "    pred_list: List[List[str]] = measure_df[pred_col].to_list()\n",
        "    ground_true_list: List[List[str]] = measure_df[ground_true_col].to_list()\n",
        "    for pred, g_true in zip(pred_list, ground_true_list):\n",
        "        apks.append(apk(g_true, pred, k=12))\n",
        "    return np.mean(apks)"
      ],
      "metadata": {
        "id": "ZuIhiE9wZsI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Thực hiện dự đoán và đánh giá mô hình"
      ],
      "metadata": {
        "id": "a8yCjp_AFVEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####XGBRanker model"
      ],
      "metadata": {
        "id": "cY4826wLfpzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make file for evaluation\n",
        "valid_sub = valid.loc[:, [\"customer_id\"]]\n",
        "\n",
        "# evaluation\n",
        "valid['preds'] = xgb_ranker.predict(valid_X)\n",
        "\n",
        "c_id2predicted_article_ids = (\n",
        "    valid\n",
        "    .sort_values(['customer_id', 'preds'], ascending=False)\n",
        "    .groupby('customer_id')['article_id'].apply(list).to_dict()\n",
        ")\n",
        "\n",
        "preds = []\n",
        "for c_id in valid_sub.customer_id:\n",
        "    pred = c_id2predicted_article_ids.get(c_id, [])\n",
        "    preds.append(pred[:12]) #tối đa 12 sản phẩm\n",
        "\n",
        "preds = [' '.join([str(p) for p in ps]) for ps in preds]\n",
        "valid_sub['prediction'] = preds\n",
        "\n",
        "valid_ground_true = neg_sampling.valid_trans.groupby(\n",
        "    'customer_id', as_index=False\n",
        ").agg(ground_true=('article_id', list))\n",
        "\n",
        "valid_measure_df = valid_sub[['customer_id', 'prediction']]\n",
        "valid_measure_df = valid_measure_df.merge(valid_ground_true, on='customer_id', how='inner')\n",
        "valid_measure_df['prediction'] = [pred.split(' ') for pred in list(valid_measure_df['prediction'].values)]\n",
        "valid_mapk_xgb = mapk(valid_measure_df, pred_col='prediction', ground_true_col='ground_true', k=12)\n",
        "valid_mapk_xgb"
      ],
      "metadata": {
        "id": "B7KBLJw14LKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db19912-52a2-4d16-af9c-bc3faa5a8f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03707456781871971"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}